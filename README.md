  # AVM-SLAM: Semantic Visual SLAM with Multi-Sensor Fusion in a Bird’s Eye View for Automated Valet Parking

This is the dataset website for our paper "[AVM-SLAM: Semantic Visual SLAM with Multi-Sensor Fusion in a Bird’s Eye View for Automated Valet Parking](https://arxiv.org/abs/2309.08180)".


## AVM-SLAM Dataset

![Dateset Infomation](https://github.com/yale-cv/avm-slam/blob/main/img/information.png)

To validate the proposed AVM-SLAM system, tests were conducted in a 220m×110m underground garage with over 430 parking spots using a test vehicle equipped with four surround-view fisheye cameras, four wheel encoders, and an IMU, all synchronized and calibrated offline.

The dataset encompasses:
- Four fisheye image sequences (10Hz@1280x960)
- One Bird's Eye View (BEV) image sequence (10Hz@1354x1632, 1.05cm/pixel) generated by our Around View Monitor (AVM) subsystem
- Four sets of wheel encoder data
- One IMU dataset

This dataset will be beneficial for further research in SLAM, especially for autonomous vehicle localization in underground garages. For access to the dataset, please contact the authors for details.

This dataset is only for academeic use under the GNU General Public License Version 3 (GPLv3). For commercial purposes, please contact the authors for details.
